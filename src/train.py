from datetime import datetime
from feast import FeatureStore, Entity, FeatureView, Field # Added Entity, FeatureView, Field for direct definition if needed
from mlflow.models.signature import infer_signature
import argparse
import yaml
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import joblib
import mlflow
import mlflow.sklearn
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK
import json
import os
import sys
# Will implement feast, mlflow, model training and hyper parameter tuning 
# Ensure the feast directory is in the Python path
print("main code for feast, mlflow,model training")
sys.path.append(os.path.join(os.getcwd(), "feast"))
try:
    from iris_features import iris_view, flower # Assuming 'flower' is your entity
except ImportError as e:
    print(f"Error importing Feast features: {e}. Make sure feast/iris_features.py exists and defines 'iris_view' and 'flower'.")
    sys.exit(1)


# ----------------- ARGUMENT PARSING -----------------
parser = argparse.ArgumentParser()
parser.add_argument("--data", default="data/iris.parquet")
parser.add_argument("--model_out", default="models/model.joblib")
args = parser.parse_args()

# ----------------- MLFLOW SETUP -----------------
# IMPORTANT: Verify this IP address. It might change if your VM reboots.
# Use 'curl ifconfig.me' on your VM to get the current external IP.
mlflow.set_tracking_uri("http://34.66.42.6:8100/")
mlflow.set_experiment("Iris-DecisionTree-Tuning")

# ----------------- LOAD INPUT DATA -----------------
# Assuming iris.parquet is already generated by an upstream DVC step
# with 'species' column and potentially others needed for Feast entity_df.
df = pd.read_parquet(args.data)
print("[DEBUG] Initial df columns:", df.columns.tolist())
print("[DEBUG] Initial df head:\n", df.head())

# Add entity_id and timestamp. Feast requires these for historical feature retrieval.
df["flower_id"] = df.index.astype(int) # Using index as unique ID for each flower
df["event_timestamp"] = datetime.utcnow() # Use a consistent timestamp for all rows if not event-specific
df["event_timestamp"] = pd.to_datetime(df["event_timestamp"], utc=True)
print("[DEBUG] df with flower_id and event_timestamp columns:\n", df.head())


# ----------------- FEATURE STORE -----------------
# Define Feast repo path (use absolute path for robustness)
feast_repo_path = os.path.abspath("feast")
print(f"[INFO] Feast repo path: {feast_repo_path}")

if not os.path.exists(os.path.join(feast_repo_path, "feature_store.yaml")):
    print(f"[ERROR] feature_store.yaml not found at {os.path.join(feast_repo_path, 'feature_store.yaml')}")
    sys.exit(1)

store = FeatureStore(repo_path=feast_repo_path)
print("[INFO] FeatureStore initialized successfully.")

# Apply Feast definitions (can be removed if feast apply is always run by DVC stage)
# If you run into "registry locked" errors, comment this out and rely on DVC's feast_apply stage.
try:
    store.apply([flower, iris_view]) # Ensure 'flower' is the correct entity name from iris_features.py
    print("[INFO] Feast definitions applied (if not already).")
except Exception as e:
    print(f"[WARN] Failed to apply Feast definitions in train.py (might be due to locked registry if already applied by DVC): {e}")


# Materialize incremental is typically for populating the online store for real-time inference.
# For offline training, get_historical_features directly reads from the configured offline source.
# You can often remove this line for offline training workflows if your FileSource is correct.
# store.materialize_incremental(end_date=datetime.utcnow()) # Consider commenting out for offline training


# ----------------- FETCH OFFLINE FEATURES FROM FEAST -----------------
# Step 1: Define entity dataframe (flower_id + timestamp only)
entity_df = df[["flower_id", "event_timestamp"]]
print("[DEBUG] entity_df for Feast:\n", entity_df.head())


# Step 2: Specify features to fetch from Feast
features_to_fetch = [
    "iris_features:sepal_length",
    "iris_features:sepal_width",
    "iris_features:petal_length",
    "iris_features:petal_width",
]

# Step 3: Retrieve offline features
print("[INFO] Fetching historical features from Feast...")
features_df = store.get_historical_features(
    entity_df=entity_df,
    features=features_to_fetch
).to_df()

print("[INFO] Feast offline features retrieved. Columns:", features_df.columns.tolist())
print("[DEBUG] Feast offline features head:\n", features_df.head())
print("[DEBUG] Feast offline features info:")
features_df.info()

# Check for NaNs in fetched features (critical for model performance)
if features_df.isnull().sum().sum() > 0:
    print("[ERROR] NaNs found in fetched features_df! Check Feast data source and definitions.")
    print(features_df.isnull().sum())
    # You might want to handle NaNs, e.g., features_df.fillna(0, inplace=True) or drop them
    # For Iris, there shouldn't be NaNs if the source data is clean.

# Step 4: Encode labels (target = species)
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
df["target"] = encoder.fit_transform(df[["species"]]).astype(int)
print("[DEBUG] df with encoded target:\n", df.head())
print("[DEBUG] Unique encoded targets:", df["target"].unique())


# Step 5: Align features and target using merge on the entity ID and timestamp
# This is the most critical change for correct data alignment.
# Ensure 'flower_id' and 'event_timestamp' are present in both DataFrames for merging.
# Feast's get_historical_features typically returns the join keys, so they should be in features_df.
full_df = pd.merge(
    features_df,
    df[["flower_id", "event_timestamp", "target", "species"]], # Include 'species' for verification if needed
    on=["flower_id", "event_timestamp"],
    how="inner" # Use inner join to ensure only perfectly matched rows are kept
)

print("[INFO] Combined features + target (after merge). Columns:", full_df.columns.tolist())
print("[DEBUG] Combined features + target head:\n", full_df.head())
print("[DEBUG] Combined features + target info:")
full_df.info()


# ----------------- PREPARE FEATURES & LABELS FOR MODEL TRAINING -----------------
# Drop unused columns for model training
# Ensure these columns exist in full_df before dropping.
columns_to_drop = ["flower_id", "event_timestamp", "target", "species"]
X = full_df.drop(columns=[col for col in columns_to_drop if col in full_df.columns])
y = full_df["target"]

# Verify X and y shapes and content
print(f"[DEBUG] X shape: {X.shape}, y shape: {y.shape}")
print("[DEBUG] X head:\n", X.head())
print("[DEBUG] y head:\n", y.head())

if X.empty or y.empty:
    print("[ERROR] X or y is empty after feature preparation. Check data alignment.")
    sys.exit(1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y # Stratify for balanced classes in split
)

print(f"[DEBUG] X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
print(f"[DEBUG] y_train value counts:\n{y_train.value_counts()}")
print(f"[DEBUG] y_test value counts:\n{y_test.value_counts()}")


# ----------------- BASELINE MODEL (for quick sanity check) -----------------
baseline = DecisionTreeClassifier(random_state=42)
baseline.fit(X_train, y_train)
baseline_preds = baseline.predict(X_test)
baseline_acc = accuracy_score(y_test, baseline_preds)
print(f"[BASELINE] Model Accuracy: {baseline_acc:.4f}")

# ----------------- HYPERPARAMETER SEARCH SPACE -----------------
# Load hyperparameters from params.yaml (though hyperopt defines its own space)
with open("params.yaml", "r") as f:
    config_params = yaml.safe_load(f) # Renamed to avoid confusion with hyperopt 'params'

space = {
    "max_depth": hp.choice("max_depth", range(2, 10)),
    "min_samples_split": hp.choice("min_samples_split", range(2, 10)),
}
print(f"[INFO] Hyperopt search space defined: {space}")


# ----------------- HYPEROPT TRAINING -----------------
best_model = None
best_acc = -1.0 # Initialize with a value that any accuracy will exceed

def objective(params):
    global best_model, best_acc # Declare global to modify
    with mlflow.start_run(nested=True):
        model = DecisionTreeClassifier(**params, random_state=42) # Add random_state for reproducibility
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        acc = accuracy_score(y_test, preds)

        mlflow.log_params(params)
        mlflow.log_metric("accuracy", acc)

        print(f"[Trial] acc={acc:.4f}, params={params}")

        # Update best model if current trial is better
        if acc > best_acc:
            best_acc = acc
            best_model = model # Store the actual model object

        input_example = X_test.iloc[:1]
        signature = infer_signature(X_test, preds)
        mlflow.sklearn.log_model(model, "model", signature=signature, input_example=input_example)

        return {"loss": -acc, "status": STATUS_OK}

print("[INFO] Starting Hyperopt optimization...")
trials = Trials()
best_hyperopt_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)

print(f"[INFO] Hyperopt finished. Best parameters found: {best_hyperopt_params}")
print(f"[INFO] Best accuracy found during trials: {best_acc:.4f}")


# ----------------- SAVE BEST MODEL -----------------
if best_model is not None:
    joblib.dump(best_model, args.model_out)
    print(f"[INFO] Best model saved to {args.model_out}")
else:
    print("[WARN] No best model identified or saved. Check Hyperopt results.")

# ----------------- LOG FINAL METRIC FOR DVC -----------------
# Use the best_acc tracked during hyperopt
final_accuracy = float(best_acc)
with open("metrics.json", "w") as f:
    json.dump({"accuracy": final_accuracy}, f)
print(f"[INFO] Final accuracy logged to metrics.json: {final_accuracy:.4f}")
